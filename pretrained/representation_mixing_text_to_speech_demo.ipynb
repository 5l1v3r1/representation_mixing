{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Representation Mixing demo",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "gpHKxLNszi8W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Representation Mixing \n",
        "The representation mixing method is described in detail in this [arxiv paper](https://arxiv.org/abs/1811.07240), and sample comparison to baselines can be seen [here](https://s3.amazonaws.com/representation-mixing-site/index.html). \n",
        "\n",
        "This notebook shows a simple example of inference using a model trained with representation mixing, and the specifics of choosing what type of input and input mask to feed the model.\n",
        "\n",
        "Models were trained on the [LJSpeech dataset](https://keithito.com/LJ-Speech-Dataset/). Special thanks to Ryuichi Yamamoto for inspiration to make a Colab notebook demo based on the [Tacotron 2 + WaveNet example](https://colab.research.google.com/github/r9y9/Colaboratory/blob/master/Tacotron2_and_WaveNet_text_to_speech_demo.ipynb) as part of the [blog post](https://r9y9.github.io/blog/2018/05/20/tacotron2/), and also for the pretrained conditional WaveNet used in the paper.\n",
        "\n",
        "**Total runtime for two minimal sentences, fast output:** ~3 minutes\n",
        "\n",
        "\n",
        "**Total runtime for two minimal sentences, WaveGlow output:** ~6 minutes"
      ]
    },
    {
      "metadata": {
        "id": "m7R_1MpFc3Za",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ]
    },
    {
      "metadata": {
        "id": "NlLC7Q7Us8go",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Author: Kyle Kastner\n",
        "# License: BSD 3-Clause\n",
        "\n",
        "import os\n",
        "from os.path import exists, join, expanduser\n",
        "import IPython\n",
        "from IPython.display import Audio\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('classic')\n",
        "\n",
        "\n",
        "os.chdir(os.path.expanduser(\"~\"))\n",
        "\n",
        "representation_mixing_dir = \"representation_mixing\"\n",
        "if not os.path.exists(representation_mixing_dir):\n",
        "  ! git clone https://github.com/kastnerkyle/$representation_mixing_dir\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KBFfji_Avluz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Install dependencies - use the default TF for now, but if we get version errors 1.6.0 is \n",
        "# what I used to train everything\n",
        "#! pip uninstall tensorflow-gpu\n",
        "#! pip uninstall tensorflow\n",
        "#! pip install -q --upgrade \"tensorflow<=1.6.0\"\n",
        "! pip install -q --upgrade \"unidecode\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iZsAP7srBBTe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "os.chdir(representation_mixing_dir)\n",
        "os.chdir(\"pretrained\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tI0jgEZrKdsN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Imports and Model Download\n",
        "\n",
        "Here we setup necessary imports, then download and unpack the pretrained model if it isn't already present."
      ]
    },
    {
      "metadata": {
        "id": "15p8phXx6nxe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from collections import namedtuple\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "import time\n",
        "from audio import soundsc\n",
        "from audio import stft\n",
        "from audio import iterate_invert_spectrogram\n",
        "from transform_text import transform_text\n",
        "from transform_text import inverse_transform_text\n",
        "from scipy.io import wavfile\n",
        "\n",
        "\n",
        "dl_link = \"https://www.dropbox.com/s/domlnxdk7wetqr3/representation_mixing_model.tar.gz?raw=1\"\n",
        "model_path = \"model_final_blended_maskfix_dirtyattnandhidconnect_softplusstep_1scale_pt925drop/models/model-512000\"\n",
        "\n",
        "if not os.path.exists(model_path.split(os.sep)[0]):\n",
        "    print(\"Downloading model from {}\".format(dl_link))\n",
        "    import urllib\n",
        "    urllib.urlretrieve (dl_link, \"representation_mixing_model.tar.gz\")\n",
        "    import tarfile\n",
        "    tar = tarfile.open(\"representation_mixing_model.tar.gz\", \"r:gz\")\n",
        "    tar.extractall()\n",
        "    tar.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wVjZCG5jLClz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Hyperparameters and Configs\n",
        "\n",
        "Now we set up general configs for pretrained model."
      ]
    },
    {
      "metadata": {
        "id": "NOQM9I1KK-J8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "random_state = np.random.RandomState(1999)\n",
        "# change this if you change the runtime type\n",
        "# CPU sampling is MUCH faster than GPU here, at least in my tests\n",
        "#config = tf.ConfigProto(\n",
        "#    device_count={'GPU': 0}\n",
        "#)\n",
        "config = None\n",
        "\n",
        "# The following hyperparameters shouldn't be changed, they match pretrained model\n",
        "# and data settings.\n",
        "batch_size = 64\n",
        "seq_len = 256\n",
        "window_mixtures = 10\n",
        "enc_units = 128\n",
        "dec_units = 512\n",
        "emb_dim = 15\n",
        "sample_rate = 22050\n",
        "window_size = 512\n",
        "step = 128\n",
        "n_mel = 80\n",
        "\n",
        "# These hyperparameters can be modified\n",
        "# For example try sonify_iter = 1000 for higher quality output\n",
        "sonify_iter = 100\n",
        "gl_window = 512\n",
        "gl_step = 32\n",
        "gl_iter = 100\n",
        "\n",
        "\n",
        "# dummy batch sizes from validation iterator in training code\n",
        "# mels = (256, 64, 80)\n",
        "# mel_mask = (256, 64)\n",
        "# text = (145, 64, 1)\n",
        "# text_mask = (145, 64)\n",
        "# mask = (145, 64)\n",
        "# mask_mask = (145, 64)\n",
        "# reset = (64, 1)\n",
        "mels = np.zeros((seq_len, batch_size, n_mel))\n",
        "mel_mask = np.ones_like(mels[..., 0])\n",
        "text = np.zeros((145, batch_size, 1))\n",
        "text_mask = np.ones_like(text[..., 0])\n",
        "mask = np.ones((145, batch_size))\n",
        "mask_mask = np.ones_like(mask)\n",
        "reset = np.zeros((batch_size, 1))\n",
        "\n",
        "\n",
        "# pre-calculated per-dimension mean and std for mel data from the training iterator, read from a file\n",
        "d = np.load(\"norm-mean-std-txt-cleanenglish_cleanersenglish_phone_cleaners-logmel-wsz512-wst128-leh125-ueh7800-nmel80.npz\")\n",
        "saved_std = d[\"std\"]\n",
        "saved_mean = d[\"mean\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "km1SAASEcIL6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Input Text to Synthesize\n",
        "Enter whatever sentences you like in the lines below (below 'cat << EOS > sample_lines.txt\n",
        "' but above 'EOS'), I put some minimal and boring examples for demonstration. Sampling time will be longer for more sentences, and also for longer sentences. You can put up to 64 sentences if you want, but it will take longer to sample."
      ]
    },
    {
      "metadata": {
        "id": "tU1lz6PcbXut",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cat << EOS > sample_lines.txt\n",
        "The cat ate bread.\n",
        "Cumulonimbus clouds are not dead.\n",
        "EOS\n",
        "\n",
        "cat sample_lines.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "37PWGXhclOpA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Representation Mixing for Inference\n",
        "Now we need to choose the input representation to use for representation mixing.\n",
        "\n",
        "Lines like ''@dh@ah @k@ae@t @ey@t @b@r@eh@d\", using the \"@\" symbol represent phonemes, and should have an all 1s mask.\n",
        "\n",
        "Lines like \"the cat ate bread.\" represent text, and should have an all 0s mask.\n",
        "\n",
        "Lines like \"the @k@ae@t ate @b@r@eh@d\" are mixed, and would have a mask made of both 0s and 1s.\n",
        "\n",
        "You can make these manually, or use the ```transform_text``` function with the ```symbol_processing``` argument set to create the sentence and the mask automatically (using cmudict and a custom function for pronunciation).\n",
        "\n",
        "```\"blended_pref\"``` symbol processing is denoted **PWCB** in the paper, and means to use CMUDict pronunciations wherever possible, but fall back to characters if no pronunciation is found (for example, Cumulonimbus in the demo sentence). Note that the first found entry of CMUDict is used for pronunciation, this can lead to issues with words like \"desert\" and \"wind\"."
      ]
    },
    {
      "metadata": {
        "id": "wi2O9scBny7x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Uncomment these lines if you want to see the char and phone symbols\n",
        "#from symbols import char_symbols\n",
        "#print(char_symbols)\n",
        "#from symbols import phone_symbols\n",
        "#print([\"@\" + p for p in phone_symbols])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GYQkrRCAjDK3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#symbol_processing = \"chars_only\"\n",
        "#symbol_processing = \"phones_only\"\n",
        "symbol_processing = \"blended_pref\"\n",
        "\n",
        "pre_lines = []\n",
        "post_lines = []\n",
        "int_lines = []\n",
        "masks = []\n",
        "\n",
        "with open(\"sample_lines.txt\", \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "lines = [l.strip() for l in lines]\n",
        "print(\"lines to sample, format 'before : after transformation'\")\n",
        "for l in lines:\n",
        "    if \".\" in l:\n",
        "        # The training set rarely has multi-sentence inputs\n",
        "        # thus having a period in the middle causes some issues\n",
        "        # a simple fix is to replace periods in the middle of the input with\n",
        "        # commas instead\n",
        "        lm1 = l[:-1].replace(\".\", \",\")\n",
        "        l = lm1 + l[-1]\n",
        "    pre_lines.append(l)\n",
        "    tt, mm = transform_text(l, symbol_processing=symbol_processing)\n",
        "    ot = inverse_transform_text(tt, mm)\n",
        "    # cut off eos\n",
        "    int_lines.append(tt[:-1])\n",
        "    masks.append(mm[:-1])\n",
        "    post_lines.append(ot[:-1])\n",
        "    print(\"{} : {}\".format(l, ot[:-1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HH1m8prEvZnl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Sample from the attention model\n",
        "\n",
        "With the inputs and masks formatted, it is time to actually sample from the model. This cell will sample until the attention center reaches the end of the sequence for every input sentence, then run for a little more to ensure the end is not cut off (~.2 seconds).\n",
        "\n",
        "This should be fairly quick (~1 minute) if you are using the demo inputs. Runtime will scale with the length of the longest sentence since we sample a batch all together."
      ]
    },
    {
      "metadata": {
        "id": "ltTj9UDRuI6K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print_every = 20\n",
        "\n",
        "if len(int_lines) > batch_size:\n",
        "    raise ValueError(\"More lines to test ({}) than batch_size ({}), needs multi-batch support!\".format(len(int_lines), len(batch_size)))\n",
        "longest = max([len(il) for il in int_lines])\n",
        "text = np.zeros((longest, batch_size, 1))\n",
        "text_mask = np.zeros((longest, batch_size))\n",
        "mask = np.zeros((longest, batch_size, 1))\n",
        "mask_mask = np.zeros((longest, batch_size))\n",
        "for n, il in enumerate(int_lines):\n",
        "    text[:len(il), n, 0] = il\n",
        "    text_mask[:len(il), n] = 1.\n",
        "    mask[:len(il), n, 0] = masks[n]\n",
        "    mask_mask[:len(il), n] = 1.\n",
        "n_to_sample = len(int_lines)\n",
        "\n",
        "start_time = time.time()\n",
        "with tf.Session(config=config) as sess:\n",
        "    saver = tf.train.import_meta_graph(model_path + '.meta')\n",
        "    saver.restore(sess, model_path)\n",
        "    fields = [\"mels\",\n",
        "              \"mel_mask\",\n",
        "              \"in_mels\",\n",
        "              \"in_mel_mask\",\n",
        "              \"out_mels\",\n",
        "              \"out_mel_mask\",\n",
        "              \"text\",\n",
        "              \"text_mask\",\n",
        "              \"mask\",\n",
        "              \"mask_mask\",\n",
        "              \"bias\",\n",
        "              \"cell_dropout\",\n",
        "              \"prenet_dropout\",\n",
        "              \"bn_flag\",\n",
        "              \"pred\",\n",
        "              \"att_w_init\",\n",
        "              \"att_k_init\",\n",
        "              \"att_h_init\",\n",
        "              \"att_c_init\",\n",
        "              \"h1_init\",\n",
        "              \"c1_init\",\n",
        "              \"h2_init\",\n",
        "              \"c2_init\",\n",
        "              \"att_w\",\n",
        "              \"att_k\",\n",
        "              \"att_phi\",\n",
        "              \"att_h\",\n",
        "              \"att_c\",\n",
        "              \"h1\",\n",
        "              \"c1\",\n",
        "              \"h2\",\n",
        "              \"c2\"]\n",
        "    vs = namedtuple('Params', fields)(\n",
        "        *[tf.get_collection(name)[0] for name in fields]\n",
        "    )\n",
        "    att_w_init = np.zeros((batch_size, 2 * enc_units))\n",
        "    att_k_init = np.zeros((batch_size, window_mixtures))\n",
        "    att_h_init = np.zeros((batch_size, dec_units))\n",
        "    att_c_init = np.zeros((batch_size, dec_units))\n",
        "    h1_init = np.zeros((batch_size, dec_units))\n",
        "    c1_init = np.zeros((batch_size, dec_units))\n",
        "    h2_init = np.zeros((batch_size, dec_units))\n",
        "    c2_init = np.zeros((batch_size, dec_units))\n",
        "\n",
        "    # zero out the mel storage to ensure no leakage\n",
        "    in_mels = 0. * mels[:1]\n",
        "    in_mel_mask = 0. * mel_mask[:1] + 1.\n",
        "\n",
        "    preds = []\n",
        "    att_ws = []\n",
        "    att_phis = []\n",
        "    is_finished_sampling = [False] * n_to_sample\n",
        "    finished_at = 100000000\n",
        "    finished_step = [-1] * n_to_sample\n",
        "\n",
        "    ii = 0\n",
        "\n",
        "    # add ~.2 sec to the end to ensure it doesn't cut off early\n",
        "    # sample_rate * .2 / fft_step\n",
        "    # min_part to account for the last window\n",
        "    min_part = window_size / float(sample_rate)\n",
        "    extra_steps = max(0, int((sample_rate * (.2 - min_part)) / float(step)))\n",
        "    keep_printing = True\n",
        "    while True:\n",
        "        if ii % print_every == 0 and keep_printing:\n",
        "            print(\"pred step {}\".format(ii))\n",
        "        feed = {\n",
        "                vs.in_mels: in_mels,\n",
        "                vs.in_mel_mask: in_mel_mask,\n",
        "                vs.bn_flag: 1.,\n",
        "                vs.text: text,\n",
        "                vs.text_mask: text_mask,\n",
        "                vs.mask: mask,\n",
        "                vs.mask_mask: mask_mask,\n",
        "                vs.cell_dropout: 1.,\n",
        "                vs.att_w_init: att_w_init,\n",
        "                vs.att_k_init: att_k_init,\n",
        "                vs.att_h_init: att_h_init,\n",
        "                vs.att_c_init: att_c_init,\n",
        "                vs.h1_init: h1_init,\n",
        "                vs.c1_init: c1_init,\n",
        "                vs.h2_init: h2_init,\n",
        "                vs.c2_init: c2_init}\n",
        "        outs = [vs.att_w, vs.att_k,\n",
        "                vs.att_h, vs.att_c,\n",
        "                vs.h1, vs.c1, vs.h2, vs.c2,\n",
        "                vs.att_phi, vs.pred]\n",
        "        r = sess.run(outs, feed_dict=feed)\n",
        "        att_w_np = r[0]\n",
        "        att_k_np = r[1]\n",
        "        att_h_np = r[2]\n",
        "        att_c_np = r[3]\n",
        "        h1_np = r[4]\n",
        "        c1_np = r[5]\n",
        "        h2_np = r[6]\n",
        "        c2_np = r[7]\n",
        "        att_phi_np = r[8]\n",
        "        pred_np = r[9]\n",
        "\n",
        "        ii += 1\n",
        "        max_text = max([text_mask[:, mbi].sum() for mbi in range(n_to_sample)])\n",
        "        if ii > 30 * max_text:\n",
        "            # it's gone too far, kill\n",
        "            finished_step = [int(30 * max_text)] * n_to_sample\n",
        "            print(\"Exceeded 30 * max text length of {},  terminating...\".format(max_text))\n",
        "            break\n",
        "\n",
        "        att_ws.append(att_w_np[0])\n",
        "        att_phis.append(att_phi_np[0])\n",
        "        preds.append(pred_np[0])\n",
        "\n",
        "        # set next inits and input values\n",
        "        in_mels[0] = pred_np\n",
        "        att_w_init = att_w_np[-1]\n",
        "        att_k_init = att_k_np[-1]\n",
        "        att_h_init = att_h_np[-1]\n",
        "        att_c_init = att_c_np[-1]\n",
        "        h1_init = h1_np[-1]\n",
        "        c1_init = c1_np[-1]\n",
        "        h2_init = h2_np[-1]\n",
        "        c2_init = c2_np[-1]\n",
        "\n",
        "        for mbi in range(n_to_sample):\n",
        "            last_sym = int(text_mask[:, mbi].sum()) - 1\n",
        "            if np.argmax(att_phi_np[0, mbi]) >= last_sym or np.argmax(att_phi_np[0, mbi]) == text_mask.shape[0]:\n",
        "                if is_finished_sampling[mbi] == False:\n",
        "                    is_finished_sampling[mbi] = True\n",
        "                    finished_step[mbi] = ii\n",
        "\n",
        "        if all(is_finished_sampling) and keep_printing:\n",
        "            keep_printing = False\n",
        "            print(\"All samples finished at step {}\".format(finished_at))\n",
        "            print(\"Extra padding {} finishing at {}\".format(extra_steps, ii + extra_steps))\n",
        "        elif keep_printing:\n",
        "            # should assign until all are finished\n",
        "            finished_at = ii\n",
        "\n",
        "        if ii > (finished_at + extra_steps):\n",
        "            print(\"Extra padding {} finished at step {}\".format(extra_steps, ii))\n",
        "            break\n",
        "end_time = time.time()\n",
        "preds = np.array(preds)\n",
        "att_ws = np.array(att_ws)\n",
        "att_phis = np.array(att_phis)\n",
        "print(\"Total time spent in RNN sampling {}\".format(end_time - start_time))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "la7CJj5P-uZE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Visualizing the Output\n",
        "What does the output from the model look like? We can plot both the attention, and the mel output to see what is going on.\n",
        "\n",
        "First, we define a convenience plotter function."
      ]
    },
    {
      "metadata": {
        "id": "Kt-guQkfvH33",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def implot(arr, axarr, axis_off=True, scale=None, title=\"\", interpolation=None,\n",
        "           cmap=None, autoaspect=True):\n",
        "    mag = arr\n",
        "    # Transpose so time is X axis, and invert y axis so\n",
        "    # frequency is low at bottom\n",
        "    mag = mag.T\n",
        "    if interpolation == None:\n",
        "        pltr = axarr.matshow\n",
        "    else:\n",
        "        pltr = axarr.imshow\n",
        "    if cmap != None:\n",
        "        pltr(mag, cmap=cmap, origin=\"lower\", interpolation=interpolation)\n",
        "    else:\n",
        "        pltr(mag, origin=\"lower\", interpolation=interpolation)\n",
        "    if axis_off:\n",
        "        plt.axis(\"off\")\n",
        "    \n",
        "    if autoaspect:\n",
        "        x1 = mag.shape[0]\n",
        "        y1 = mag.shape[1]\n",
        "        if scale == \"specgram\":\n",
        "            y1 = int(y1 * .20)\n",
        "\n",
        "        def _autoaspect(x_range, y_range):\n",
        "            \"\"\"\n",
        "            The aspect to make a plot square with ax.set_aspect in Matplotlib\n",
        "            \"\"\"\n",
        "            mx = max(x_range, y_range)\n",
        "            mn = min(x_range, y_range)\n",
        "            if x_range <= y_range:\n",
        "                return mx / float(mn)\n",
        "            else:\n",
        "                return mn / float(mx)\n",
        "        asp = _autoaspect(x1, y1)\n",
        "        axarr.set_aspect(asp)\n",
        "    plt.title(title)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vw2Qgif7ABMH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we can plot the attention, and the mel spectrogram. Change  ```sentence``` to change which example (from your defined sentences) is viewed.\n"
      ]
    },
    {
      "metadata": {
        "id": "g3pB51di__wc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sentence = 1\n",
        "\n",
        "format_lines = []\n",
        "for pl in post_lines:\n",
        "    parts = pl.split(\" \")\n",
        "    chunks = [[\"@\" + pi for pi in p.split(\"@\")[1:]] if \"@\" in p else p for p in parts]\n",
        "    fill = [\" \"] * len(chunks)\n",
        "    out = []\n",
        "    for ii in range(len(chunks)):\n",
        "        out.append(chunks[ii])\n",
        "        out.append([fill[ii]])\n",
        "    # cut trailing space\n",
        "    out = out[:-1]\n",
        "    out_flat = [item for sublist in out for item in sublist]\n",
        "    format_lines.append(out_flat)\n",
        "\n",
        "spectrogram = preds[:, sentence] * saved_std + saved_mean\n",
        "f, axarr = plt.subplots(1, 1)\n",
        "implot(spectrogram[:finished_step[sentence]], axarr, axis_off=False,\n",
        "       scale=\"specgram\", cmap=\"viridis\")\n",
        "axarr.set_ylabel(\"Mel-Freq Bin\")\n",
        "axarr.set_xlabel(\"Time (sample frames)\")\n",
        "plt.figure()\n",
        "\n",
        "phi_i = att_phis[:, sentence]\n",
        "f, axarr = plt.subplots(1, 1)\n",
        "implot(phi_i[:finished_step[sentence], :len(masks[sentence])], axarr,\n",
        "       axis_off=False, cmap=\"gray\")\n",
        "axarr.set_ylabel(\"Symbols\")\n",
        "axarr.set_xlabel(\"Time (sample frames)\")\n",
        "axarr.yaxis.set_major_locator(plt.FixedLocator(range(len(masks[sentence]))))\n",
        "ticks = axarr.set_yticklabels([c for c in format_lines[sentence]])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "odgmKOTYPZJe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Audio Utilities\n",
        "This cell defines two convenience functions to approximately convert the mel spectrogram into a waveform."
      ]
    },
    {
      "metadata": {
        "id": "MTPg90sEu_fI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def logmel(waveform):\n",
        "    z = tf.contrib.signal.stft(waveform, window_size, step)\n",
        "    magnitudes = tf.abs(z)\n",
        "    filterbank = tf.contrib.signal.linear_to_mel_weight_matrix(\n",
        "        num_mel_bins=n_mel,\n",
        "        num_spectrogram_bins=magnitudes.shape[-1].value,\n",
        "        sample_rate=sample_rate,\n",
        "        lower_edge_hertz=125.,\n",
        "        upper_edge_hertz=7800.)\n",
        "    melspectrogram = tf.tensordot(magnitudes, filterbank, 1)\n",
        "    return tf.log1p(melspectrogram)\n",
        "\n",
        "def sonify(spectrogram, samples, transform_op_fn, logscaled=True):\n",
        "    # Very special thanks to Carl Thome for showing this technique\n",
        "    # https://twitter.com/carlthome/status/1002187555700396035\n",
        "    # All credit to him, any bugs are my own\n",
        "    graph = tf.Graph()\n",
        "    with graph.as_default():\n",
        "\n",
        "        noise = tf.Variable(tf.random_normal([samples], stddev=1e-6))\n",
        "\n",
        "        x = transform_op_fn(noise)\n",
        "        y = spectrogram\n",
        "\n",
        "        if logscaled:\n",
        "            x = tf.expm1(x)\n",
        "            y = tf.expm1(y)\n",
        "\n",
        "        # tf.nn.normalize arguments changed between versions...\n",
        "        def normalize(a):\n",
        "            return a / tf.sqrt(tf.maximum(tf.reduce_sum(a ** 2, axis=0), 1E-12))\n",
        "\n",
        "        x = normalize(x)\n",
        "        y = normalize(y)\n",
        "        tf.losses.mean_squared_error(x, y[-tf.shape(x)[0]:])\n",
        "\n",
        "        optimizer = tf.contrib.opt.ScipyOptimizerInterface(\n",
        "            loss=tf.losses.get_total_loss(),\n",
        "            var_list=[noise],\n",
        "            tol=1e-16,\n",
        "            method='L-BFGS-B',\n",
        "            options={\n",
        "                'maxiter': sonify_iter,\n",
        "                'disp': False\n",
        "            })\n",
        "\n",
        "    # THIS REALLY SHOULDN'T RUN ON GPU BUT SEEMS TO?\n",
        "    config = tf.ConfigProto(\n",
        "        device_count={'CPU' : 1, 'GPU' : 0},\n",
        "        allow_soft_placement=True,\n",
        "        log_device_placement=False\n",
        "        )\n",
        "    with tf.Session(config=config, graph=graph) as session:\n",
        "        session.run(tf.global_variables_initializer())\n",
        "        optimizer.minimize(session)\n",
        "        waveform = session.run(noise)\n",
        "    return waveform"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FqWCjImf9ZAq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Two-stage L-BFGS + GL Waveforms\n",
        "\n",
        "With the utility code defined above, we can run the two-stage sampling pipeline. \n",
        "\n",
        "This will take ~1 minute for the demo sentences, and will take longer for more sentences. In theory this could be parallelized over multiple cores, or machines but the simple version here is easier to work with."
      ]
    },
    {
      "metadata": {
        "id": "L_kJ1hOAKZQ1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pre_time = 0\n",
        "post_time = 0\n",
        "joint_time = 0\n",
        "for jj in range(n_to_sample):\n",
        "    # use extra steps from earlier\n",
        "    pjj = preds[:(finished_step[jj] + extra_steps), jj]\n",
        "    spectrogram = pjj * saved_std + saved_mean\n",
        "    mel_dump = \"sample_{}_mels.npz\".format(jj)\n",
        "    np.savez(mel_dump, mels=spectrogram)\n",
        "    prename = \"sample_{}_pre.wav\".format(jj)\n",
        "\n",
        "    this_time = time.time()\n",
        "\n",
        "    reconstructed_waveform = sonify(spectrogram, len(spectrogram) * step, logmel)\n",
        "    end_this_time = time.time()\n",
        "    wavfile.write(prename, sample_rate, soundsc(reconstructed_waveform))\n",
        "    elapsed = end_this_time - this_time\n",
        "\n",
        "    print(\"Elapsed pre sampling time {} s\".format(elapsed))\n",
        "    pre_time += elapsed\n",
        "    joint_time += elapsed\n",
        "\n",
        "    fftsize = gl_window\n",
        "    substep = gl_step\n",
        "    postname = \"sample_{}_post.wav\".format(jj)\n",
        "    this_time = time.time()\n",
        "\n",
        "    rw_s = np.abs(stft(soundsc(reconstructed_waveform).astype(\"float64\"), fftsize=fftsize, step=substep, real=False,\n",
        "                       compute_onesided=False))\n",
        "    rw = iterate_invert_spectrogram(rw_s, fftsize, substep, n_iter=gl_iter, verbose=False)\n",
        "    end_this_time = time.time()\n",
        "    wavfile.write(postname, sample_rate, soundsc(rw))\n",
        "    elapsed = end_this_time - this_time\n",
        "\n",
        "    print(\"Elapsed post sampling time {} s\".format(elapsed))\n",
        "    post_time += elapsed\n",
        "    joint_time += elapsed\n",
        "print(\"Combined pre time {} s\".format(pre_time))\n",
        "print(\"Combined post time {} s\".format(post_time))\n",
        "print(\"Combined joint time {} s\".format(joint_time))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hNG8oI4OiJkJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## L-BFGS + GL Audio Samples\n",
        "\n",
        "Now we can listen to the samples, after the two-stage pipeline. Not bad! But perhaps we can do better..."
      ]
    },
    {
      "metadata": {
        "id": "OIyfhn0v9Ntg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open(\"sample_lines.txt\", \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "lines = [l.strip() for l in lines]\n",
        "\n",
        "def sort(files):\n",
        "    return sorted(files, key=lambda k: int(k.split(\"_\")[1]))\n",
        "    \n",
        "mel_files = sort([f for f in os.listdir(\".\") if \"_mels.npz\" in f])\n",
        "audio_files = sort([f for f in os.listdir(\".\") if \"_post.wav\" in f])          \n",
        "maps = zip(lines, mel_files[:len(lines)], audio_files[:len(lines)])\n",
        "\n",
        "for idx, (text, mel, audio) in enumerate(maps):\n",
        "  print(idx, text)\n",
        "  IPython.display.display(Audio(audio, rate=22050))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "txEuh_rCx4Vb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##BONUS: WaveGlow Sampling\n",
        "\n",
        "After the paper on representation mixing was finalized, NVidia released [WaveGlow](https://github.com/NVIDIA/waveglow). Alongside, [FloWaveNet](https://github.com/ksw0306/FloWaveNet) (a very similar idea) was also released.\n",
        "\n",
        "These are both *much* faster ways to create high quality audio than a standard autoregressive conditional WaveNet, so we demonstrate output with this instead.\n",
        "\n",
        "However the quality is currently not quite as good as WaveNet, perhaps because WaveGlow is more sensitive to the quality and scale of the input conditioning and wasn't trained on the outputs of my model. Specifically, there seems to be some kind of distortion after WaveGlow sampling when going directly from the waveforms heard above. \n",
        "\n",
        "This will need more work, but take this as a simple demonstration of using WaveGlow at least.\n",
        "\n",
        "Ultimately, the choice of \"neural vocoder\" is a design choice, which is largely independent of the frontend - a frontend trained with representation mixing should be compatible with nearly any \"neural vocoder\" or DSP inversion routine."
      ]
    },
    {
      "metadata": {
        "id": "dWAqm7tUx6Wq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if not os.path.exists(\"waveglow\"):\n",
        "    !git clone https://github.com/NVIDIA/waveglow.git\n",
        "    os.chdir(\"waveglow\")\n",
        "    !git submodule init\n",
        "    !git submodule update\n",
        "    os.chdir(\"..\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s1lOkfXNP-DM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! pip3 install librosa\n",
        "# this will be 1.0 once there is a pip-able version\n",
        "! pip3 install torch_nightly -f https://download.pytorch.org/whl/nightly/cu92/torch_nightly.html\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7aIat1cPUwHy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "waveglow_model = \"https://drive.google.com/file/d/1cjKPHbtAMh_4HTHmuIGNkbOkPBD9qwhj/view?usp=sharing\"\n",
        "waveglow_model_id = waveglow_model.split(\"//\")[1].split(\"/\")[3]\n",
        "waveglow_mels = \"https://drive.google.com/file/d/1g_VXK2lpP9J25dQFhQwx7doWl_p20fXA/view?usp=sharing\"\n",
        "waveglow_mels_id = waveglow_mels.split(\"//\")[1].split(\"/\")[3]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x91YUiatWGrh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# from https://stackoverflow.com/questions/25010369/wget-curl-large-file-from-google-drive/39225039#39225039\n",
        "import requests\n",
        "\n",
        "def download_file_from_google_drive(id, destination):\n",
        "    def get_confirm_token(response):\n",
        "        for key, value in response.cookies.items():\n",
        "            if key.startswith('download_warning'):\n",
        "                return value\n",
        "\n",
        "        return None\n",
        "\n",
        "    def save_response_content(response, destination):\n",
        "        CHUNK_SIZE = 32768\n",
        "\n",
        "        with open(destination, \"wb\") as f:\n",
        "            for chunk in response.iter_content(CHUNK_SIZE):\n",
        "                if chunk: # filter out keep-alive new chunks\n",
        "                    f.write(chunk)\n",
        "\n",
        "    URL = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "    session = requests.Session()\n",
        "\n",
        "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
        "    token = get_confirm_token(response)\n",
        "\n",
        "    if token:\n",
        "        params = { 'id' : id, 'confirm' : token }\n",
        "        response = session.get(URL, params = params, stream = True)\n",
        "\n",
        "    save_response_content(response, destination)    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p4o9yl3oWTE3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "os.chdir(\"waveglow\")\n",
        "if not os.path.exists(\"waveglow_old.pt\"):\n",
        "    download_file_from_google_drive(waveglow_model_id, \"waveglow_old.pt\")\n",
        "os.chdir(\"..\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0iUH_CqMWlAx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "os.chdir(\"waveglow\")\n",
        "# test example, from the NVidia repo\n",
        "if not os.path.exists(\"mel_spectrograms.zip\"):\n",
        "    download_file_from_google_drive(waveglow_mels_id, \"mel_spectrograms.zip\")\n",
        "    ! unzip mel_spectrograms.zip\n",
        "os.chdir(\"..\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YhSB5tnyqKnR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Sampling with WaveGlow\n",
        "Do a bit of name cleanup, and move the mel spectrograms into the ``samples_temp`` directory.\n",
        "\n",
        "There are many warnings about `'nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
        "'` , but for now we can ignore them."
      ]
    },
    {
      "metadata": {
        "id": "EwIVn0XWZ5FB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "os.chdir(\"waveglow\")\n",
        "# bug? requires train files.txt\n",
        "!touch train_files.txt\n",
        "!rm *.wav\n",
        "!python3 mel2samp.py -f <(ls ../sample*.wav) -o . -c config.json\n",
        "!mkdir -p samples_temp\n",
        "!rm samples_temp/*.pt\n",
        "!for qq in *.wav.pt; do echo $(basename \"$qq\" .wav.pt); mv \"$qq\" samples_temp/$(basename \"$qq\" .wav.pt).pt; done\n",
        "!python3 inference.py -f <(ls samples_temp/*.pt) -w waveglow_old.pt -o . --is_fp16 -s 0.6\n",
        "os.chdir(\"..\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KYlmAacezf1e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## WaveGlow Audio Samples\n",
        "With this complete, we can listen to the samples after WaveGlow synthesis. There is some distortion, which may be due to the pretrained WaveGlow not being trained on this output directly."
      ]
    },
    {
      "metadata": {
        "id": "JAMznHKwy1an",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"WaveGlow samples\")\n",
        "with open(\"sample_lines.txt\", \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "lines = [l.strip() for l in lines]\n",
        "\n",
        "def sort(files):\n",
        "    return sorted(files, key=lambda k: int(k.split(\"_\")[1]))\n",
        "    \n",
        "audio_files = sort([\"waveglow/\" + f for f in os.listdir(\"waveglow\") if \"post\" in f])          \n",
        "maps = zip(lines, audio_files[:len(lines)])\n",
        "\n",
        "for idx, (text, audio) in enumerate(maps):\n",
        "  print(idx, text)\n",
        "  IPython.display.display(Audio(audio, rate=22050))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7OGKGJZk0xSD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If you made it this far, congratulations! Have fun with the notebook, and thanks for reading.\n",
        "\n",
        "kk"
      ]
    }
  ]
}
